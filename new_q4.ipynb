{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df = pd.read_csv('stock_data.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values with the median of the column\n",
    "df.fillna(df.select_dtypes(include=[np.number]).median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#Visulaize the closing price history\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Close Price History')\n",
    "plt.plot(df['Close'])\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Close Price USD ($)', fontsize=14)\n",
    "plt.grid()\n",
    "plt.savefig('close_price_history.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#Visualize the trading volume history\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Volume History')\n",
    "plt.plot(df['Volume'])\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Volume', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.savefig('trading_volume_history.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#Correlation matrix\n",
    "corr = df.corr()\n",
    "plt.figure(figsize=(16,8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig('correlation_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#Analyze trends and seasonality\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Fill missing values in 'Close' column\n",
    "df['Close'] = df['Close'].interpolate(method='linear')\n",
    "\n",
    "decompose = seasonal_decompose(df['Close'], model='multiplicative', period=20)\n",
    "fig = decompose.plot()\n",
    "fig.set_size_inches(16,8)\n",
    "plt.savefig('seasonal_decompose.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#Check for stationarity\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "result = adfuller(df['Close'])\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print(f'{key}:{value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate moving averages\n",
    "df['MA5'] = df['Close'].rolling(window=5).mean()\n",
    "df['MA20'] = df['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Moving average crossovers (buy/sell signals)\n",
    "df['MA5_20_Cross'] = (df['MA5'] > df['MA20']).astype(int)\n",
    "df['MA20_50_Cross'] = (df['MA20'] > df['MA50']).astype(int)\n",
    "\n",
    "# Distance from moving averages (normalized)\n",
    "df['Dist_MA20'] = (df['Close'] - df['MA20']) / df['MA20']\n",
    "df['Dist_MA50'] = (df['Close'] - df['MA50']) / df['MA50']\n",
    "\n",
    "# Calculate price momentum\n",
    "df['Momentum'] = df['Close'] - df['Close'].shift(5)\n",
    "df['Momentum_10'] = df['Close'] - df['Close'].shift(10)\n",
    "df['Momentum_20'] = df['Close'] - df['Close'].shift(20)\n",
    "\n",
    "# Calculate volatility (standard deviation over a window)\n",
    "df['Volatility'] = df['Close'].rolling(window=20).std()\n",
    "\n",
    "# Price rate of change at different timeframes\n",
    "df['ROC_5'] = df['Close'].pct_change(periods=5) * 100\n",
    "df['ROC_10'] = df['Close'].pct_change(periods=10) * 100\n",
    "df['ROC_20'] = df['Close'].pct_change(periods=20) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN values created by the indicators\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Visualize the new features\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.title('Technical Indicators')\n",
    "plt.plot(df['Close'], label='Close')\n",
    "plt.plot(df['MA20'], label='MA20')\n",
    "plt.plot(df['MA50'], label='MA50')\n",
    "plt.legend()\n",
    "plt.savefig('technical_indicators.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Visualize MACD\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.title('MACD')\n",
    "plt.plot(df['MACD'], label='MACD')\n",
    "plt.plot(df['MACD_Signal'], label='Signal Line')\n",
    "plt.legend()\n",
    "plt.savefig('macd.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prepaeration for LSTM -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable: 5-day future closing price\n",
    "df['Target'] = df['Close'].shift(-5)  # 5 days into the future\n",
    "df.dropna(inplace=True)  # Drop rows with NaN targets\n",
    "\n",
    "# Select features\n",
    "features = [ 'Close', 'High','Low','Volume','MA5', 'MA20', 'Momentum', 'Volatility', 'RSI', 'MACD',\n",
    "            'MACD_Signal', 'MA5_20_Cross', 'MA20_50_Cross', 'Dist_MA20', 'Dist_MA50',\n",
    "            'ROC_5', 'ROC_10', 'ROC_20']\n",
    "X = df[features].values\n",
    "y = df['Target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (keeping time order)\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data \n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_scaled = y_scaler.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for LSTM input (lookback window)\n",
    "def create_sequences(X, y, time_steps=20):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        X_seq.append(X[i:i + time_steps])\n",
    "        y_seq.append(y[i + time_steps])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "time_steps = 20  # Number of previous time steps to use as input\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, time_steps)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Check the shapes\n",
    "print(f\"X_train_seq shape: {X_train_seq.shape}\")  # (samples, time steps, features)\n",
    "print(f\"y_train_seq shape: {y_train_seq.shape}\")  # (samples, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1))  # Predicting a single value (close price)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Print input shapes to debug\n",
    "print(f\"X_train_seq shape: {X_train_seq.shape}\")\n",
    "print(f\"y_train_seq shape: {y_train_seq.shape}\")\n",
    "\n",
    "# Rebuild the model with proper input shape\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('model_training_history.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Check shapes\n",
    "print(f\"y_pred_scaled shape: {y_pred_scaled.shape}\")\n",
    "print(f\"y_test_seq shape: {y_test_seq.shape}\")\n",
    "\n",
    "# If the model is outputting multiple values per sample, take only the first one\n",
    "if len(y_pred_scaled.shape) > 2 or (len(y_pred_scaled.shape) == 2 and y_pred_scaled.shape[1] > 1):\n",
    "\ty_pred_scaled = y_pred_scaled[:, 0].reshape(-1, 1)\n",
    "\n",
    "# Inverse transform to get actual price values\n",
    "y_pred = y_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_actual = y_scaler.inverse_transform(y_test_seq)\n",
    "\n",
    "# Calculate error metrics\n",
    "rmse = math.sqrt(mean_squared_error(y_test_actual, y_pred))\n",
    "mae = mean_absolute_error(y_test_actual, y_pred)\n",
    "mape = np.mean(np.abs((y_test_actual - y_pred) / y_test_actual)) * 100\n",
    "\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Mean Absolute Percentage Error: {mape}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Calculate directional accuracy (whether the price direction was predicted correctly)\n",
    "y_test_direction = np.diff(y_test_actual.flatten())\n",
    "y_pred_direction = np.diff(y_pred.flatten())\n",
    "directional_accuracy = np.mean((y_test_direction > 0) == (y_pred_direction > 0)) * 100\n",
    "print(f'Directional Accuracy: {directional_accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using N-Beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preparation and Normalization\n",
    "# -------------------------------------\n",
    "# Select the target column (e.g., 'Close' price for stocks)\n",
    "target_column = 'Close'\n",
    "data = df[[target_column]].copy()\n",
    "\n",
    "# Apply normalization\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# 2. Create Sequences for Time Series Modeling\n",
    "# -------------------------------------------\n",
    "def create_sequences(data, lookback, forecast_horizon):\n",
    "    \"\"\"\n",
    "    Create input/output sequences for time series forecasting\n",
    "    \n",
    "    Parameters:\n",
    "    data: normalized data array\n",
    "    lookback: number of past time steps to use as input\n",
    "    forecast_horizon: number of future time steps to predict\n",
    "    \n",
    "    Returns:\n",
    "    X: input sequences of shape (samples, lookback)\n",
    "    y: output sequences of shape (samples, forecast_horizon)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - lookback - forecast_horizon + 1):\n",
    "        X.append(data[i:(i + lookback)].flatten())\n",
    "        y.append(data[(i + lookback):(i + lookback + forecast_horizon)].flatten())\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define sequence parameters\n",
    "lookback = 30       # Use 30 days of historical data for prediction\n",
    "forecast_horizon = 5 # Predict 5 days into the future\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sequences(scaled_data, lookback, forecast_horizon)\n",
    "print(f\"Input sequence shape: {X.shape}\")\n",
    "print(f\"Output sequence shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# 3. Train-Test Split\n",
    "# ------------------\n",
    "# Use the last 20% of data for testing to preserve temporal order\n",
    "split_idx = int(len(X) * 0.8)\n",
    "\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create N-BEATS Block Building Function\n",
    "# ----------------------------------------\n",
    "def nbeats_block(x, units, theta_dim, share_weights=False, block_name=None):\n",
    "    \"\"\"\n",
    "    Create an N-BEATS block\n",
    "    \n",
    "    Parameters:\n",
    "    x: input tensor\n",
    "    units: number of hidden units\n",
    "    theta_dim: dimension of theta layer\n",
    "    share_weights: whether to share weights\n",
    "    block_name: name of the block\n",
    "    \n",
    "    Returns:\n",
    "    backcast: backcast output tensor\n",
    "    forecast: forecast output tensor\n",
    "    \"\"\"\n",
    "    # Block contains 4 fully connected layers\n",
    "    for _ in range(4):\n",
    "        x = Dense(units, activation='relu')(x)\n",
    "    \n",
    "    # Output is split into backcast and forecast\n",
    "    theta_b = Dense(theta_dim, name=f'{block_name}_theta_b')(x)\n",
    "    theta_f = Dense(theta_dim, name=f'{block_name}_theta_f')(x)\n",
    "    \n",
    "    # Reshape to match input shape for backcast and forecast_horizon for forecast\n",
    "    backcast = Dense(lookback, name=f'{block_name}_backcast')(theta_b)\n",
    "    forecast = Dense(forecast_horizon, name=f'{block_name}_forecast')(theta_f)\n",
    "    \n",
    "    return backcast, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# 5. Build the N-BEATS Model\n",
    "# -------------------------\n",
    "from tensorflow.keras.layers import Input, Dense, Subtract, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_nbeats_model(lookback, forecast_horizon, stacks=2, blocks_per_stack=3, units=256, share_weights=False):\n",
    "    \"\"\"\n",
    "    Build the N-BEATS model\n",
    "    \n",
    "    Parameters:\n",
    "    lookback: input sequence length\n",
    "    forecast_horizon: output sequence length\n",
    "    stacks: number of stacks\n",
    "    blocks_per_stack: number of blocks per stack\n",
    "    units: number of hidden units\n",
    "    share_weights: whether to share weights between blocks\n",
    "    \n",
    "    Returns:\n",
    "    model: compiled N-BEATS model\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(lookback,), name='input_layer')\n",
    "    \n",
    "    # Initial backcast is the input\n",
    "    backcast = input_layer\n",
    "    forecast = None\n",
    "    \n",
    "    # For collecting forecasts from each block\n",
    "    forecasts = []\n",
    "    \n",
    "    # Stack loop\n",
    "    for stack_id in range(stacks):\n",
    "        # Block loop\n",
    "        for block_id in range(blocks_per_stack):\n",
    "            # Apply N-BEATS block\n",
    "            backcast_block, forecast_block = nbeats_block(\n",
    "                backcast, \n",
    "                units=units, \n",
    "                theta_dim=lookback,\n",
    "                share_weights=share_weights,\n",
    "                block_name=f'stack_{stack_id}_block_{block_id}'\n",
    "            )\n",
    "            \n",
    "            # Collect forecasts\n",
    "            forecasts.append(forecast_block)\n",
    "            \n",
    "            # Subtract the backcast from the input to get the residual\n",
    "            backcast = Subtract(name=f'subtract_{stack_id}_{block_id}')([backcast, backcast_block])\n",
    "    \n",
    "    # Double check stack and blocks_per_stack\n",
    "    assert len(forecasts) == stacks * blocks_per_stack\n",
    "    \n",
    "    # Sum all the forecasts\n",
    "    if len(forecasts) > 1:\n",
    "        forecast = forecasts[0]\n",
    "        for i in range(1, len(forecasts)):\n",
    "            forecast = Concatenate(axis=1, name=f'forecast_add_{i}')([forecast, forecasts[i]])\n",
    "    else:\n",
    "        forecast = forecasts[0]\n",
    "    \n",
    "    # Build model\n",
    "    model = Model(inputs=input_layer, outputs=forecast)\n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_nbeats_model(\n",
    "    lookback=lookback,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    stacks=2,\n",
    "    blocks_per_stack=3,\n",
    "    units=128\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# 6. Model Training\n",
    "# ---------------\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Add, Lambda\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check shapes\n",
    "print(f\"X shape: {X_train.shape}\")\n",
    "print(f\"y shape: {y_train.shape}\")\n",
    "\n",
    "# For the N-BEATS model, we need to ensure the target has the correct shape\n",
    "# Take only the first forecast step for now (we can use more as needed)\n",
    "y_train_reshaped = y_train[:, 0].reshape(-1, 1)\n",
    "print(f\"Reshaped y shape: {y_train_reshaped.shape}\")\n",
    "\n",
    "# Rebuild the model with clear TF graph to avoid variable creation issues\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Recreate the model using the Functional API more explicitly\n",
    "input_layer = Input(shape=(lookback,), name='input_layer')\n",
    "forecasts = []\n",
    "\n",
    "# Initial backcast is the input\n",
    "backcast = input_layer\n",
    "\n",
    "# Build stacks and blocks\n",
    "for stack_id in range(2):  # 2 stacks\n",
    "    for block_id in range(3):  # 3 blocks per stack\n",
    "        # First dense layers\n",
    "        x = Dense(128, activation='relu')(backcast)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        \n",
    "        # Output layers\n",
    "        theta_b = Dense(lookback, name=f'stack_{stack_id}_block_{block_id}_theta_b')(x)\n",
    "        theta_f = Dense(lookback, name=f'stack_{stack_id}_block_{block_id}_theta_f')(x)\n",
    "        \n",
    "        backcast_block = Dense(lookback, name=f'stack_{stack_id}_block_{block_id}_backcast')(theta_b)\n",
    "        forecast_block = Dense(forecast_horizon, name=f'stack_{stack_id}_block_{block_id}_forecast')(theta_f)\n",
    "        \n",
    "        # Collect forecast\n",
    "        forecasts.append(forecast_block)\n",
    "        \n",
    "        # Residual connection\n",
    "        backcast = Subtract(name=f'subtract_{stack_id}_{block_id}')([backcast, backcast_block])\n",
    "\n",
    "# Average the forecasts from all blocks\n",
    "if len(forecasts) > 1:\n",
    "    final_forecast = forecasts[0]\n",
    "    for i in range(1, len(forecasts)):\n",
    "        final_forecast = Add(name=f'forecast_add_{i}')([final_forecast, forecasts[i]])\n",
    "    final_forecast = Lambda(lambda x: x / len(forecasts))(final_forecast)\n",
    "else:\n",
    "    final_forecast = forecasts[0]\n",
    "\n",
    "# Create and compile model\n",
    "model = Model(inputs=input_layer, outputs=final_forecast)\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Set up callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=0.0001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train_reshaped,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# 8. Model Evaluation\n",
    "# -----------------\n",
    "# Import r2_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions and actual values to original scale\n",
    "y_test_orig = scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n",
    "y_pred_orig = scaler.inverse_transform(y_pred.reshape(-1, 1)).reshape(y_pred.shape)\n",
    "\n",
    "# Calculate error metrics on original scale data\n",
    "mse = mean_squared_error(y_test_orig, y_pred_orig)\n",
    "rmse = math.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_orig, y_pred_orig)\n",
    "r2 = r2_score(y_test_orig.reshape(-1), y_pred_orig.reshape(-1))\n",
    "\n",
    "print(\"\\nModel Evaluation Metrics (Original Scale):\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error)\n",
    "mape = np.mean(np.abs((y_test_orig - y_pred_orig) / y_test_orig)) * 100\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# # Calculate and Visualize Directional Accuracy\n",
    "# # -------------------------------------------\n",
    "\n",
    "# # First, make predictions if not already done\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Reshape if needed for directional accuracy calculation\n",
    "# y_test_reshaped = y_test if y_test.ndim == 2 else y_test.reshape(-1, forecast_horizon)\n",
    "# y_pred_reshaped = y_pred if y_pred.ndim == 2 else y_pred.reshape(-1, forecast_horizon)\n",
    "\n",
    "# # Inverse transform to original scale if required\n",
    "# y_test_orig = scaler.inverse_transform(y_test_reshaped.reshape(-1, 1)).reshape(y_test_reshaped.shape)\n",
    "# y_pred_orig = scaler.inverse_transform(y_pred_reshaped.reshape(-1, 1)).reshape(y_pred_reshaped.shape)\n",
    "\n",
    "# # Calculate directional accuracy for each forecast horizon\n",
    "# directional_accuracy = np.zeros(forecast_horizon)\n",
    "\n",
    "# for horizon in range(forecast_horizon):\n",
    "#     # Get the actual price movements (up or down)\n",
    "#     # We compare consecutive predictions for the same horizon \n",
    "#     actual_direction = np.sign(np.diff(y_test_orig[1:, horizon] - y_test_orig[:-1, horizon]))\n",
    "#     predicted_direction = np.sign(np.diff(y_pred_orig[1:, horizon] - y_pred_orig[:-1, horizon]))\n",
    "    \n",
    "#     # Calculate accuracy (ignoring zero changes)\n",
    "#     non_zero_indices = actual_direction != 0\n",
    "#     if np.sum(non_zero_indices) > 0:\n",
    "#         correct_directions = np.sum((actual_direction[non_zero_indices] == predicted_direction[non_zero_indices]))\n",
    "#         directional_accuracy[horizon] = correct_directions / np.sum(non_zero_indices)\n",
    "\n",
    "# # Print overall directional accuracy\n",
    "# overall_da = np.mean(directional_accuracy)\n",
    "# print(f\"\\nDirectional Accuracy Analysis:\")\n",
    "# print(f\"Overall Directional Accuracy: {overall_da:.4f} ({overall_da*100:.2f}%)\")\n",
    "\n",
    "# for h in range(forecast_horizon):\n",
    "#     print(f\"Horizon {h+1} Directional Accuracy: {directional_accuracy[h]:.4f} ({directional_accuracy[h]*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Visualize predictions vs actual values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.plot(y_test_actual, label='Actual Prices')\n",
    "plt.plot(y_pred, label='Predicted Prices')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.savefig('prediction_vs_actual.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate trading performance\n",
    "initial_capital = 10000\n",
    "position = 0\n",
    "portfolio_value = [initial_capital]\n",
    "returns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple trading strategy: Buy when predicted price increases, sell when predicted price decreases\n",
    "for i in range(1, len(y_pred)):\n",
    "    # Predicted price change\n",
    "    price_change = y_pred[i] - y_pred[i-1]\n",
    "    \n",
    "    if price_change > 0 and position == 0:  # Buy signal\n",
    "        position = portfolio_value[-1] / y_test_actual[i]\n",
    "        portfolio_value.append(portfolio_value[-1])  # No change until we sell\n",
    "    elif price_change < 0 and position > 0:  # Sell signal\n",
    "        portfolio_value.append(position * y_test_actual[i])\n",
    "        position = 0\n",
    "    else:\n",
    "        if position > 0:  # If holding position, update portfolio value\n",
    "            portfolio_value.append(position * y_test_actual[i])\n",
    "        else:\n",
    "            portfolio_value.append(portfolio_value[-1])\n",
    "    \n",
    "    returns.append((portfolio_value[-1] - portfolio_value[-2]) / portfolio_value[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Calculate trading metrics\n",
    "cumulative_return = (portfolio_value[-1] - initial_capital) / initial_capital * 100\n",
    "annualized_return = (1 + (portfolio_value[-1] - initial_capital) / initial_capital) ** (252 / len(y_pred)) - 1\n",
    "sharpe_ratio = np.mean([r.item() if isinstance(r, np.ndarray) else r for r in returns]) / np.std([r.item() if isinstance(r, np.ndarray) else r for r in returns]) * np.sqrt(252)\n",
    "max_drawdown = np.max(np.maximum.accumulate([pv.item() if isinstance(pv, np.ndarray) else pv for pv in portfolio_value]) - [pv.item() if isinstance(pv, np.ndarray) else pv for pv in portfolio_value]) / np.max([pv.item() if isinstance(pv, np.ndarray) else pv for pv in portfolio_value])\n",
    "\n",
    "# Extract scalar values from numpy arrays\n",
    "cumulative_return = cumulative_return.item() if isinstance(cumulative_return, np.ndarray) else cumulative_return\n",
    "annualized_return = annualized_return.item() if isinstance(annualized_return, np.ndarray) else annualized_return\n",
    "sharpe_ratio = sharpe_ratio.item() if isinstance(sharpe_ratio, np.ndarray) else sharpe_ratio\n",
    "max_drawdown = max_drawdown.item() if isinstance(max_drawdown, np.ndarray) else max_drawdown\n",
    "\n",
    "print(f'Cumulative Return: {cumulative_return:.2f}%')\n",
    "print(f'Annualized Return: {annualized_return:.2f}%')\n",
    "print(f'Sharpe Ratio: {sharpe_ratio:.2f}')\n",
    "print(f'Maximum Drawdown: {max_drawdown:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Convert any numpy arrays in portfolio_value to scalar values\n",
    "portfolio_value = [pv.item() if isinstance(pv, np.ndarray) else pv for pv in portfolio_value]\n",
    "\n",
    "# Plot portfolio value over time\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.title('Portfolio Value')\n",
    "plt.plot(portfolio_value)\n",
    "plt.xlabel('Trading Days')\n",
    "plt.ylabel('Portfolio Value ($)')\n",
    "plt.grid(True)\n",
    "plt.savefig('portfolio_performance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# For LSTM, direct feature importance is not straightforward\n",
    "# One approach is to use permutation importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# We need to adapt this for LSTM's sequential data\n",
    "# Create a function to make predictions and calculate RMSE\n",
    "def predict_and_score(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    return -mean_squared_error(y, y_pred)  # Negative because we want higher values to be more important\n",
    "\n",
    "# Baseline score\n",
    "baseline_score = predict_and_score(model, X_test_seq, y_test_seq)\n",
    "\n",
    "# Initialize importance scores\n",
    "importance_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# For each feature, permute its values and measure the impact\n",
    "for i in range(X_train_seq.shape[2]):\n",
    "    # Make a copy of the test data\n",
    "    X_permuted = X_test_seq.copy()\n",
    "    \n",
    "    # Shuffle the values of the current feature across all sequences\n",
    "    for j in range(X_test_seq.shape[0]):\n",
    "        np.random.shuffle(X_permuted[j, :, i])\n",
    "    \n",
    "    # Calculate new score with permuted feature\n",
    "    new_score = predict_and_score(model, X_permuted, y_test_seq)\n",
    "    \n",
    "    # The importance is the difference in scores\n",
    "    importance = baseline_score - new_score\n",
    "    importance_scores[features[i]] = importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Normalize importances\n",
    "max_importance = max(importance_scores.values())\n",
    "for feature in importance_scores:\n",
    "    importance_scores[feature] /= max_importance\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(list(importance_scores.keys()), list(importance_scores.values()))\n",
    "plt.xlabel('Normalized Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
